{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating words from N-gram.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOmIaPsPNOW3aUPslyE95Pr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palakagl/NLP/blob/main/Generating_words_from_N_gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Natural Language Processing using Python\n",
        "\n",
        "# N-Gram Modelling - Word Grams\n",
        "# Importing libraries\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF-LotshZrPV",
        "outputId": "83eda6ba-8668-4bf6-b32a-89d0dbf0db11"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKHKwHBpZoTd",
        "outputId": "4912fb9f-059c-45da-cea5-6dabf17240e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple Inc. is an American multinational technology company that specializes in consumer electronics , software and online services . Apple is the largest information technology company by revenue ( totaling US $ 365.8 billion in 2021 ) and , as of January 2021 , it is the world 's most valuable company ,\n"
          ]
        }
      ],
      "source": [
        "# Sample data\n",
        "#text = \"\"\"Global warming or climate change has become a worldwide concern. It is gradually developing into an unprecedented environmental crisis evident in melting glaciers, changing weather patterns, rising sea levels, floods, cyclones and droughts. Global warming implies an increase in the average temperature of the Earth due to entrapment of greenhouse gases in the earthâ€™s atmosphere.\"\"\"\n",
        "text = \"\"\"Apple Inc. is an American multinational technology company that specializes in consumer electronics, software and online services. Apple is the largest information technology company by revenue (totaling US$365.8 billion in 2021) and, as of January 2021, it is the world's most valuable company, the fourth-largest personal computer vendor by unit sales and second-largest mobile phone manufacturer.\"\"\"\n",
        "\n",
        "# Order of the grams\n",
        "n = 3\n",
        "\n",
        "# Our N-Grams\n",
        "ngrams = {}\n",
        "\n",
        "# Building the model\n",
        "words = nltk.word_tokenize(text)\n",
        "for i in range(len(words)-n):\n",
        "    gram = ' '.join(words[i:i+n])\n",
        "    if gram not in ngrams.keys():\n",
        "        ngrams[gram] = []\n",
        "    ngrams[gram].append(words[i+n])\n",
        "    \n",
        "# Testing the model\n",
        "currentGram = ' '.join(words[0:n])\n",
        "result = currentGram\n",
        "for i in range(50):\n",
        "    if currentGram not in ngrams.keys():\n",
        "        break\n",
        "    possibilities = ngrams[currentGram]\n",
        "    nextItem = possibilities[random.randrange(len(possibilities))]\n",
        "    result += ' '+nextItem\n",
        "    rWords = nltk.word_tokenize(result)\n",
        "    currentGram = ' '.join(rWords[len(rWords)-n:len(rWords)])\n",
        "\n",
        "print(result)"
      ]
    }
  ]
}